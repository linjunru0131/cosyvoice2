import os
import sys
import pandas as pd
import torch
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('/home/student/work/fangyouying/finaldesign/CosyVoice2')

# é…ç½®è·¯å¾„
TRAIN_DIR = './train'
PARQUET_DIR = os.path.join(TRAIN_DIR, 'parquet')
METADATA_PATH = os.path.join(TRAIN_DIR, 'metadata.csv')
WAVS_DIR = os.path.join(TRAIN_DIR, 'wavs')
SPK2EMBEDDING_PATH = os.path.join(TRAIN_DIR, 'spk2embedding.pt')
UTT2TOKEN_PATH = os.path.join(TRAIN_DIR, 'utt2speech_token.pt')

# åˆ›å»ºParquetç›®å½•
os.makedirs(PARQUET_DIR, exist_ok=True)

# åŠ è½½é¢„å¤„ç†ä¸­é—´æ–‡ä»¶
print("Loading spk2embedding.pt...")
spk2embedding = torch.load(SPK2EMBEDDING_PATH)
# é€‚é…å•è¯´è¯äººç‰¹å¾ï¼šæå–å”¯ä¸€çš„ç‰¹å¾å€¼
if len(spk2embedding) == 1:
    spk_emb = list(spk2embedding.values())[0]
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    if isinstance(spk_emb, torch.Tensor):
        spk_emb_np = spk_emb.numpy()
    elif isinstance(spk_emb, list):
        spk_emb_np = np.array(spk_emb)
    else:
        spk_emb_np = None
    print(f"  - å•è¯´è¯äººç‰¹å¾ï¼Œç»´åº¦ï¼š{spk_emb_np.shape if spk_emb_np is not None else 'æœªçŸ¥'}")
else:
    spk_emb_np = None
    print(f"  - å¤šè¯´è¯äººç‰¹å¾æ•°é‡ï¼š{len(spk2embedding)}")

print("Loading utt2speech_token.pt...")
utt2token = torch.load(UTT2TOKEN_PATH)
print(f"  - è¯­éŸ³Tokenæ•°é‡ï¼š{len(utt2token)}")

# è¯»å–metadata.csv
print("Reading metadata.csv...")
metadata = []
missing_token = 0
with open(METADATA_PATH, 'r', encoding='utf-8') as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        utt_id, text = line.split('|', 1)
        # åªæ£€æŸ¥Tokenæ˜¯å¦å­˜åœ¨ï¼ˆè¯´è¯äººç‰¹å¾å…±ç”¨ï¼‰
        if utt_id not in utt2token:
            missing_token += 1
            continue
        metadata.append({
            'utt_id': utt_id,
            'text': text,
            'wav_path': os.path.join(WAVS_DIR, f'{utt_id}.wav')
        })

print(f"  - åŸå§‹æ ·æœ¬æ•°ï¼š{len(metadata) + missing_token}")
print(f"  - æœ‰æ•ˆæ ·æœ¬æ•°ï¼ˆæœ‰Tokenï¼‰ï¼š{len(metadata)}")
print(f"  - ç¼ºå¤±Tokençš„æ ·æœ¬æ•°ï¼š{missing_token}")

# æŒ‰æ¯1000æ¡æ ·æœ¬ç”Ÿæˆä¸€ä¸ªParquetæ–‡ä»¶
batch_size = 1000
num_batches = (len(metadata) + batch_size - 1) // batch_size

print(f"\nGenerating {num_batches} Parquet files...")
generated_files = []
for batch_idx in tqdm(range(num_batches), desc='Generating Parquet files'):
    start = batch_idx * batch_size
    end = min((batch_idx + 1) * batch_size, len(metadata))
    batch_data = metadata[start:end]
    
    # è¡¥å……ç‰¹å¾ï¼šæ‰€æœ‰æ ·æœ¬å…±ç”¨å•è¯´è¯äººç‰¹å¾ + å„è‡ªçš„Token
    for item in batch_data:
        utt_id = item['utt_id']
        # å…±ç”¨è¯´è¯äººç‰¹å¾
        item['spk_embedding'] = spk_emb_np
        # å„è‡ªçš„è¯­éŸ³Token
        token = utt2token[utt_id]
        if isinstance(token, torch.Tensor):
            item['speech_token'] = token.numpy()
        elif isinstance(token, list):
            item['speech_token'] = np.array(token)
        else:
            item['speech_token'] = None
    
    # ç”ŸæˆParquetæ–‡ä»¶
    df = pd.DataFrame(batch_data)
    parquet_path = os.path.join(PARQUET_DIR, f'data_{batch_idx}.parquet')
    df.to_parquet(parquet_path, index=False)
    generated_files.append(parquet_path)

# ç”Ÿæˆtrain.data.list
print("\nGenerating train.data.list...")
with open('train.data.list', 'w', encoding='utf-8') as f:
    for file_path in generated_files:
        rel_path = os.path.relpath(file_path, '.')
        f.write(f'{rel_path}\n')

# æœ€ç»ˆéªŒè¯
num_parquet = len(generated_files)
num_samples = sum([len(pd.read_parquet(f)) for f in generated_files])

print(f'\nâœ… Parquetæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼')
print(f'   - Parquetç›®å½•ï¼š{PARQUET_DIR}')
print(f'   - ç”Ÿæˆæ–‡ä»¶æ•°ï¼š{num_parquet}ä¸ª')
print(f'   - æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{num_samples}æ¡')
print(f'   - æ•°æ®åˆ—è¡¨ï¼štrain.data.list (å…±{num_parquet}è¡Œ)')
print(f'\nğŸ“Œ éªŒè¯å‘½ä»¤ï¼š')
print(f'   ls -l {PARQUET_DIR} | head -5')
print(f'   cat train.data.list | head -3')